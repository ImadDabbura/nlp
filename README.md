# Natural Language Processing

## Courses

- [Standford's CS224N: Natural Language Processing with Deep
  Learning](https://web.stanford.edu/class/cs224n/index.html)
- [CMU's Advanced NLP](https://www.phontron.com/class/anlp2022/index.html)

## Resources

- [Neural Machine Translation by Jointly Learning to Align and Translate (2014)](https://arxiv.org/abs/1409.0473)
- [Attention Is All You Need (2017)](https://arxiv.org/abs/1706.03762)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)](
  https://arxiv.org/abs/1810.04805)
- [Improving Language Understanding by Generative Pre-Training (2018)](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)
  ](https://arxiv.org/abs/1910.13461)
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022)](https://arxiv.org/abs/2205.14135)
- [Cramming: Training a Language Model on a Single GPU in One Day (2022)](https://arxiv.org/abs/2212.14034)
- [Training Compute-Optimal Large Language Models (2022)](https://arxiv.org/abs/2203.15556)
-  [Training Language Models to Follow Instructions with Human Feedback (2022)](https://arxiv.org/abs/2203.02155)
- [Constitutional AI: Harmlessness from AI Feedback (2022)](https://arxiv.org/abs/2212.08073)
